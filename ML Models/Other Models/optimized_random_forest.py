# -*- coding: utf-8 -*-
"""Random Forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bdwE2HC1ROsA7fYbZw9JsaFhZp_TuWPF
"""

!pip install keras-tuner
import tensorflow as tf
from tensorflow.keras import layers, Sequential
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import pandas as pd
import numpy as np
import keras_tuner as kt
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV

# Load and preprocess dataset
df = pd.read_excel('supernova_quasar_output.xlsx')
X = df[['u', 'g', 'r', 'i', 'z']]  # Features
Y = df[['MU_SH0ES']]  # Target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, Y, test_size=0.2, random_state=42
)

def create_optimized_random_forest():
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2']
    }

    rf = RandomForestRegressor(random_state=42)
    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='r2', verbose=1)
    grid_search.fit(X_train, y_train.values.ravel())

    print(f"Best Parameters: {grid_search.best_params_}")
    print(f"Best Cross-Validation Score: {grid_search.best_score_:.4f}")

    return grid_search.best_estimator_

# Train a single model and store results
def train_model(model_func, model_id, results_dict):
    print(f"Training {model_id}...")
    if model_id == "Bayesian Regression (Naive Bayes Alternative)":
        model, y_pred, r2 = model_func()
        y_pred = np.clip(y_pred, None, 1000)  # Apply filter
        results_dict[model_id] = {
            "model": model,
            "r2": r2,
            "y_pred": y_pred,
        }
        print(f"{model_id} R²: {r2:.4f}")
    else:
        model = model_func()
        if isinstance(model, Sequential):  # TensorFlow/Keras Neural Network
            history = model.fit(X_train, y_train, epochs=50, validation_split=0.2, verbose=0)
            y_pred = model.predict(X_test).flatten()
            y_pred = np.clip(y_pred, None, 1000)  # Apply filter
            results_dict[model_id] = {
                "model": model,
                "r2": r2_score(y_test, y_pred),
                "y_pred": y_pred,
                "history": history.history,
            }

            # Plot training and validation loss
            plt.figure(figsize=(10, 6))
            plt.plot(history.history['loss'], label='Training Loss', color='orange')
            plt.plot(history.history['val_loss'], label='Validation Loss', color='blue')
            plt.xlabel('Epochs')
            plt.ylabel('Loss')
            plt.title(f'Training and Validation Loss - {model_id}')
            plt.legend()
            plt.grid(True)
            plt.show()
        else:  # Other scikit-learn models
            model.fit(X_train, y_train.values.ravel())
            y_pred = model.predict(X_test)
            y_pred = np.clip(y_pred, None, 1000)  # Apply filter
            results_dict[model_id] = {
                "model": model,
                "r2": r2_score(y_test, y_pred),
                "y_pred": y_pred,
            }
        print(f"{model_id} R²: {results_dict[model_id]['r2']:.4f}")

# Train and plot results (highlight exact matches and plot validation loss separately)
def train_and_plot_models_with_individual_visuals(models):
    results_dict = {}

    for model_func, model_id in models:
        train_model(model_func, model_id, results_dict)

    # Visualization: Separate graphs for each model
    zcmb = df.loc[y_test.index, 'zcmb'].values.flatten()
    y_actual = y_test.values.flatten()

    for model_id, result in results_dict.items():
        # Exclude predictions greater than 4000
        y_pred = result["y_pred"]
        mask = y_pred <= 4000  # Keep predictions less than or equal to 4000
        zcmb_filtered = zcmb[mask]
        y_actual_filtered = y_actual[mask]
        y_pred_filtered = y_pred[mask]

        # Plot zcmb vs Actual and Predicted MU_SH0ES
        plt.figure(figsize=(10, 6))
        plt.scatter(zcmb_filtered, y_actual_filtered, color='green', label='Actual MU_SH0ES', alpha=0.7)
        plt.scatter(zcmb_filtered, y_pred_filtered, color='red', label='Predicted MU_SH0ES', alpha=0.7)
        plt.xlabel('zcmb (Redshift)')
        plt.ylabel('MU_SH0ES')
        plt.title(f'Actual vs Predicted MU_SH0ES - {model_id}\nR²: {result["r2"]:.4f}')
        plt.legend()
        plt.grid(True)
        plt.show()

# Train and plot with visuals for each model
if __name__ == '__main__':
    models = [
        (create_optimized_random_forest, "Optimized Random Forest"),
    ]
    train_and_plot_models_with_individual_visuals(models)

"""Save results"""

import pickle

# Save results to a file after training
def save_results(results_dict, filename="optimized_randomf.pkl"):
    with open(filename, "wb") as f:
        pickle.dump(results_dict, f)
    print(f"Results saved to {filename}")

# Save the results after training
save_results(results_dict, "optimized_randomf.pkl")

# Train models and save results
def train_models_and_save(models, filename="optimized_randomf.pkl"):
    results_dict = {}

    # Train models and store results
    for model_func, model_id in models:
        train_model(model_func, model_id, results_dict)

    # Save results to a file
    with open(filename, "wb") as f:
        pickle.dump(results_dict, f)
    print(f"Results saved to {filename}")

# Call this function to train models and save results
if __name__ == '__main__':
    models = [
        (create_optimized_random_forest, "Optimized Random Forest"),

]

train_models_and_save(models, "optimized_randomf.pkl")

"""Model testi için burası"""

import pandas as pd
import joblib
import matplotlib.pyplot as plt

# Load the optimized Random Forest model
model = joblib.load("/content/drive/MyDrive/CosmoAI/optimized_randomf.pkl")  # Replace with your saved model filename

# Load the quasar dataset
data = pd.read_excel("quasar_output.xlsx")  # Replace with your dataset filename

# Replace with your feature column names (same as used for training)
features = ["u", "g", "r", "i", "z"]

# Strip column names to remove extra spaces and ensure consistency
data.columns = data.columns.str.strip()

# Check if CID column exists
if "cid" not in data.columns:
    raise KeyError("The dataset does not contain a 'CID' column. Please verify the column name.")

# Ensure dataset contains the required features
missing_features = [feature for feature in features if feature not in data.columns]
if missing_features:
    raise ValueError(f"The dataset is missing the following required features: {missing_features}")

# Group by CID and calculate predicted MU_SHOES for each supernova
results = []
for cid, group in data.groupby("cid"):
    # Predict MU_SHOES for the group of quasars
    predictions = model.predict(group[features])

    # Average the predictions for the current CID
    avg_mu_shoes = predictions.mean()

    # Get the corresponding zCMB value (assumes all rows in the group have the same zCMB)
    zcmb = group["zcmb"].iloc[0]

    # Store the result
    results.append({"cid": cid, "zcmb": zcmb, "Predicted_MU_SHOES": avg_mu_shoes})

# Convert results to a DataFrame
results_df = pd.DataFrame(results)

# Plot zCMB vs. Predicted MU_SHOES
plt.figure(figsize=(10, 6))
plt.scatter(results_df["zcmb"], results_df["Predicted_MU_SHOES"], alpha=0.7)
plt.title("Predicted MU_SHOES vs. zCMB")
plt.xlabel("zcmb")
plt.ylabel("Predicted MU_SHOES")
plt.grid(True)
plt.show()

# Save the results to a CSV file
results_df.to_csv("predicted_mu_shoes_per_cid.csv", index=False)

print("Predictions completed and saved to 'predicted_mu_shoes_per_cid.csv'.")