# -*- coding: utf-8 -*-
"""Done.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15yBjRsu-xkFHo4ffzuDEPkPqx35eGMZI
"""

!pip install keras-tuner
import tensorflow as tf
from tensorflow.keras import layers, Sequential
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import pandas as pd
import numpy as np
import keras_tuner as kt
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV

# Load and preprocess dataset
df = pd.read_excel('supernova_quasar_output.xlsx')
X = df[['u', 'g', 'r', 'i', 'z']]  # Features
Y = df[['MU_SH0ES']]  # Target

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, Y, test_size=0.2, random_state=42
)

# Define models
def create_model_2():
    def build_nn_model(hp):
        model = Sequential()
        model.add(layers.Dense(
            units=hp.Int('units', min_value=128, max_value=512, step=32),
            activation='relu',
            input_shape=(X_train.shape[1],)
        ))
        model.add(layers.Dense(1))  # Output layer
        model.compile(
            optimizer=tf.keras.optimizers.Adam(
                learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
            ),
            loss='mse',
            metrics=['mse']
        )
        return model

    tuner = kt.Hyperband(
        build_nn_model,
        objective='val_mse',
        max_epochs=10,
        directory='nn_tuner_dir',
        project_name='nn_tuning'
    )
    tuner.search(X_train, y_train, epochs=10, validation_split=0.2, verbose=0)
    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
    model = tuner.hypermodel.build(best_hps)
    return model

def create_optimized_random_forest():
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 20, 30, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2']
    }

    rf = RandomForestRegressor(random_state=42)
    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='r2', verbose=1)
    grid_search.fit(X_train, y_train.values.ravel())

    print(f"Best Parameters: {grid_search.best_params_}")
    print(f"Best Cross-Validation Score: {grid_search.best_score_:.4f}")

    return grid_search.best_estimator_

def create_model_4():
    return DecisionTreeRegressor(random_state=42)

def create_model_5():
    return KNeighborsRegressor(n_neighbors=5)

def create_model_6():
    model = BayesianRidge()
    model.fit(X_train, y_train.values.ravel())
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    return model, y_pred, r2

# Train a single model and store results
def train_model(model_func, model_id, results_dict):
    print(f"Training {model_id}...")
    if model_id == "Bayesian Regression (Naive Bayes Alternative)":
        model, y_pred, r2 = model_func()
        y_pred = np.clip(y_pred, None, 1000)  # Apply filter
        results_dict[model_id] = {
            "model": model,
            "r2": r2,
            "y_pred": y_pred,
        }
        print(f"{model_id} R²: {r2:.4f}")
    else:
        model = model_func()
        if isinstance(model, Sequential):  # TensorFlow/Keras Neural Network
            history = model.fit(X_train, y_train, epochs=50, validation_split=0.2, verbose=0)
            y_pred = model.predict(X_test).flatten()
            y_pred = np.clip(y_pred, None, 1000)  # Apply filter
            results_dict[model_id] = {
                "model": model,
                "r2": r2_score(y_test, y_pred),
                "y_pred": y_pred,
                "history": history.history,
            }

            # Plot training and validation loss
            plt.figure(figsize=(10, 6))
            plt.plot(history.history['loss'], label='Training Loss', color='orange')
            plt.plot(history.history['val_loss'], label='Validation Loss', color='blue')
            plt.xlabel('Epochs')
            plt.ylabel('Loss')
            plt.title(f'Training and Validation Loss - {model_id}')
            plt.legend()
            plt.grid(True)
            plt.show()
        else:  # Other scikit-learn models
            model.fit(X_train, y_train.values.ravel())
            y_pred = model.predict(X_test)
            y_pred = np.clip(y_pred, None, 1000)  # Apply filter
            results_dict[model_id] = {
                "model": model,
                "r2": r2_score(y_test, y_pred),
                "y_pred": y_pred,
            }
        print(f"{model_id} R²: {results_dict[model_id]['r2']:.4f}")

# Train and plot results (highlight exact matches and plot validation loss separately)
def train_and_plot_models_with_individual_visuals(models):
    results_dict = {}

    for model_func, model_id in models:
        train_model(model_func, model_id, results_dict)

    # Visualization: Separate graphs for each model
    zcmb = df.loc[y_test.index, 'zcmb'].values.flatten()
    y_actual = y_test.values.flatten()

    for model_id, result in results_dict.items():
        # Exclude predictions greater than 4000
        y_pred = result["y_pred"]
        mask = y_pred <= 4000  # Keep predictions less than or equal to 4000
        zcmb_filtered = zcmb[mask]
        y_actual_filtered = y_actual[mask]
        y_pred_filtered = y_pred[mask]

        # Plot zcmb vs Actual and Predicted MU_SH0ES
        plt.figure(figsize=(10, 6))
        plt.scatter(zcmb_filtered, y_actual_filtered, color='green', label='Actual MU_SH0ES', alpha=0.7)
        plt.scatter(zcmb_filtered, y_pred_filtered, color='red', label='Predicted MU_SH0ES', alpha=0.7)
        plt.xlabel('zcmb (Redshift)')
        plt.ylabel('MU_SH0ES')
        plt.title(f'Actual vs Predicted MU_SH0ES - {model_id}\nR²: {result["r2"]:.4f}')
        plt.legend()
        plt.grid(True)
        plt.show()

# Train and plot with visuals for each model
if __name__ == '__main__':
    models = [
        (create_model_2, "Neural Network"),
        (create_optimized_random_forest, "Optimized Random Forest"),
        (create_model_4, "Decision Tree"),
        (create_model_5, "K-Nearest Neighbors"),
        (create_model_6, "Bayesian Regression (Naive Bayes Alternative)"),
    ]
    train_and_plot_models_with_individual_visuals(models)

# Train and plot with visuals for each model
if __name__ == '__main__':
    models = [
        (create_model_2, "Neural Network"),
        (create_optimized_random_forest, "Optimized Random Forest"),
        (create_model_4, "Decision Tree"),
        (create_model_5, "K-Nearest Neighbors"),
    ]
    train_and_plot_models_with_individual_visuals(models)

"""Just Printing"""

import pickle

# Save results to a file after training
def save_results(results_dict, filename="model_results.pkl"):
    with open(filename, "wb") as f:
        pickle.dump(results_dict, f)
    print(f"Results saved to {filename}")

# Save the results after training
save_results(results_dict, "model_results.pkl")

# Train models and save results
def train_models_and_save(models, filename="model_results.pkl"):
    results_dict = {}

    # Train models and store results
    for model_func, model_id in models:
        train_model(model_func, model_id, results_dict)

    # Save results to a file
    with open(filename, "wb") as f:
        pickle.dump(results_dict, f)
    print(f"Results saved to {filename}")

# Call this function to train models and save results
if __name__ == '__main__':
    models = [
        (create_model_2, "Neural Network"),
        (create_optimized_random_forest, "Optimized Random Forest"),
        (create_model_4, "Decision Tree"),
        (create_model_5, "K-Nearest Neighbors"),
]
    train_models_and_save(models, "model_results.pkl")

# Test the custom loss model
def test_custom_loss_model(model, X_test, y_test):
    y_pred = model.predict(X_test).flatten()
    print("Custom Loss Model Test Results:")
    for actual, predicted in zip(y_test.values.flatten()[:10], y_pred[:10]):  # Display the first 10 results
        print(f"Actual: {actual:.2f}, Predicted: {predicted:.2f}")
    return y_pred

# Test the Keras Tuner model
def test_keras_tuner_model(model, X_test, y_test):
    y_pred = model.predict(X_test).flatten()
    print("Keras Tuner Model Test Results:")
    for actual, predicted in zip(y_test.values.flatten()[:10], y_pred[:10]):  # Display the first 10 results
        print(f"Actual: {actual:.2f}, Predicted: {predicted:.2f}")
    return y_pred

# Test the models
custom_loss_predictions = test_custom_loss_model(custom_loss_model, X_test, y_test)
keras_tuner_predictions = test_keras_tuner_model(keras_tuner_model, X_test, y_test)

# Plot results from saved data
def plot_from_saved_results(filename="model_results.pkl"):
    # Load results from file
    with open(filename, "rb") as f:
        results_dict = pickle.load(f)

    # Extract zcmb and actual values once
    zcmb = df.loc[y_test.index, 'zcmb'].values.flatten()
    y_actual = y_test.values.flatten()

    for model_id, result in results_dict.items():
        # Extract predictions
        y_pred = result["y_pred"]

        # Exclude predictions greater than 4000 for clarity
        mask = y_pred <= 4000
        zcmb_filtered = zcmb[mask]
        y_actual_filtered = y_actual[mask]
        y_pred_filtered = y_pred[mask]

        # Plot zcmb vs Actual (green) and Predicted (red) MU_SH0ES
        plt.figure(figsize=(10, 6))
        plt.scatter(zcmb_filtered, y_pred_filtered, color='red', alpha=0.7, label='Predicted MU_SH0ES', zorder=1)
        plt.scatter(zcmb_filtered, y_actual_filtered, color='green', alpha=0.7, label='Actual MU_SH0ES', zorder=2)
        plt.xlabel('zcmb (Redshift)')
        plt.ylabel('MU_SH0ES')
        plt.title(f'Actual vs Predicted MU_SH0ES - {model_id}\nR²: {result["r2"]:.4f}')
        plt.legend()
        plt.grid(True)
        plt.show()

# Call this function to plot graphs from saved results
plot_from_saved_results("model_results.pkl")

import pickle
import numpy as np

def filter_outlier_from_results(filename="model_results.pkl", threshold=400):
    # Load results from file
    with open(filename, "rb") as f:
        results_dict = pickle.load(f)

    # Iterate through each model's results
    for model_id, result in results_dict.items():
        y_pred = result["y_pred"]

        # Filter out values above the threshold
        valid_mask = y_pred <= threshold
        results_dict[model_id]["y_pred"] = y_pred[valid_mask]
        results_dict[model_id]["r2"] = r2_score(
            y_test.values.flatten()[valid_mask], y_pred[valid_mask]
        )

    # Save the filtered results back to the file
    with open(filename, "wb") as f:
        pickle.dump(results_dict, f)
    print(f"Outlier filtered and results saved back to {filename}")

# Call the function to filter out the outlier
filter_outlier_from_results("model_results.pkl", threshold=400)

plot_from_saved_results("model_results.pkl")

# Plot results from saved data
def plot_from_saved_results(filename="model_results.pkl"):
    # Load results from file
    with open(filename, "rb") as f:
        results_dict = pickle.load(f)

    # Extract zcmb and actual values once
    zcmb = df.loc[y_test.index, 'zcmb'].values.flatten()
    y_actual = y_test.values.flatten()

    for model_id, result in results_dict.items():
        # Extract predictions
        y_pred = result["y_pred"]

        # Ensure mask is applied to all arrays with matching dimensions
        mask = y_pred <= 4000
        zcmb_filtered = zcmb[mask]
        y_actual_filtered = y_actual[mask]
        y_pred_filtered = y_pred[mask]

        # Plot zcmb vs Actual (green) and Predicted (red) MU_SH0ES
        plt.figure(figsize=(10, 6))
        plt.scatter(zcmb_filtered, y_pred_filtered, color='red', alpha=0.7, label='Predicted MU_SH0ES', zorder=1)
        plt.scatter(zcmb_filtered, y_actual_filtered, color='green', alpha=0.7, label='Actual MU_SH0ES', zorder=2)
        plt.xlabel('zcmb (Redshift)')
        plt.ylabel('MU_SH0ES')
        plt.title(f"Actual vs Predicted MU_SH0ES - {model_id}\nR²: {result['r2']:.4f}")
        plt.legend()
        plt.grid(True)
        plt.show()

plt.title(f"Actual vs Predicted MU_SH0ES - {model_id}\nR²: {result['r2']:.4f}")